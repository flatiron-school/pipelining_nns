{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1R_sGmaAxFHx"
      },
      "source": [
        "# Neural Network Pipelines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a9EwoC5exFHz"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_sample_images\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.preprocessing import OneHotEncoder, FunctionTransformer\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "from sklearn.pipeline import Pipeline\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from tensorflow.keras.wrappers import scikit_learn\n",
        "from tensorflow.keras.callbacks import EarlyStopping"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ea-WyBm-xFH0"
      },
      "source": [
        "## Learning Objectives\n",
        "\n",
        "- Use `tensorflow` to code up a neural network model\n",
        "- Use wrappers inside `tensorflow` to make models that can jibe with `sklearn`\n",
        "- Add a `tensorflow` network into an `sklearn` pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "atRZYYm4xFH1"
      },
      "outputs": [],
      "source": [
        "digits = load_digits()\n",
        "X = digits.data\n",
        "y = digits.target"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X[0]"
      ],
      "metadata": {
        "id": "TFJlNAiHd-U_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(X[0].reshape(8, 8));"
      ],
      "metadata": {
        "id": "Bzq8Fcp5eAiT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "That's a 0!"
      ],
      "metadata": {
        "id": "bG_TfzF-eSY2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y[0]"
      ],
      "metadata": {
        "id": "xJSZwenGeTxr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tgSe1qLRxFH3"
      },
      "source": [
        "#### Getting data ready for modeling\n",
        "**Preprocessing**:\n",
        "\n",
        "- use train_test_split to create X_train, y_train, X_test, and y_test\n",
        "- Split training data into pure_train and validation sets.\n",
        "- Scale the pixel intensity to a value between 0 and 1."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i0xvahGBxFH4"
      },
      "source": [
        "Scaling our input variables will help speed up our neural network.\n",
        "\n",
        "Since our minimum intensity is 0, we can normalize the inputs by dividing each value by the max value (16)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cpTU0dKwxFH4"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test =\\\n",
        "    train_test_split(X, y, random_state=42, test_size=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test = X_train/16, X_test/16"
      ],
      "metadata": {
        "id": "EI0VlXuhmtqe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2fukH8KKxFIA"
      },
      "source": [
        "For a multiclass output, our neural net expects our target to be in a certain form."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "67h-VPozxFIA"
      },
      "outputs": [],
      "source": [
        "ohe = OneHotEncoder(sparse=False)\n",
        "y_train_enc = ohe.fit_transform(y_train.reshape(-1, 1))\n",
        "y_test_enc = ohe.transform(y_test.reshape(-1, 1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BgFwQ5zsxFIA"
      },
      "outputs": [],
      "source": [
        "y_test_enc"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To complete our model we'll want a softmax activation in the output layer:"
      ],
      "metadata": {
        "id": "xGIyZk83GrEF"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gq8ZGYw6xFIA"
      },
      "source": [
        "$$\\large \\text{Softmax}(x_{i}) = \\frac{\\exp(x_i)}{\\sum_j \\exp(x_j)}$$\n",
        "\n",
        "The sofmax function outputs a number between 0 and 1 for each of our classes.  All of the probabilities of the classes sum up to 1.\n",
        "\n",
        "The number of nodes in our output layer equals the number of categories in our dataset.\n",
        "\n",
        "We also need a new loss function: **categorical crossentropy**, which calculates a separate loss for each label and then sums the results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lyC72NE9xFIA"
      },
      "outputs": [],
      "source": [
        "# Model with ten output neurons:\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(12, activation='tanh', input_dim=64))\n",
        "model.add(Dense(8, activation='tanh'))\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "results = model.fit(X_train, y_train_enc,\n",
        "                   epochs=20, batch_size=100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jnbzhlgSxFH6"
      },
      "source": [
        "We can access the history of our model via `results.history`.\n",
        "Use __dict__ to take a tour."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sjXfoey9xFH6"
      },
      "outputs": [],
      "source": [
        "results.__dict__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y88ZaQyUxFH6"
      },
      "outputs": [],
      "source": [
        "tanh_loss = results.history['loss']\n",
        "tanh_accuracy = results.history['accuracy']\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
        "sns.lineplot(x=results.epoch, y=tanh_loss, ax=ax1, label='loss')\n",
        "sns.lineplot(x=results.epoch, y=tanh_accuracy, ax=ax2, label='accuracy');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14VTxzpPxFH6"
      },
      "source": [
        "We have two plots above both relating to the quality of our model.  The left-hand plot is our loss. It uses the probabilities associated with our predictions to judge how well our prediction fits reality. We want it to decrease as far as possible.\n",
        "\n",
        "The accuracy judges how well the predictions are after applying the threshold at the output layer.  We want accuracy to increase."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gNzY6PCCxFH7"
      },
      "source": [
        "If we look at our loss, it is still decreasing. That is a signal that our model is **still learning**. If our model is still learning, we can allow it to get better by turning a few dials.\n",
        "\n",
        "Let's:\n",
        "- increase the number of epochs;\n",
        "- change tanh activation in the hidden layers to ReLU; and\n",
        "- decrease the batch size."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_train_enc.shape"
      ],
      "metadata": {
        "id": "Iog5FEmNpijw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DaVPzOJmxFH7"
      },
      "outputs": [],
      "source": [
        "model2 = Sequential()\n",
        "model2.add(Dense(12, activation='relu', input_dim=64))\n",
        "model2.add(Dense(8, activation='relu'))\n",
        "model2.add(Dense(10, activation='softmax'))\n",
        "\n",
        "model2.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# batch_size of None means batch_size = 32!\n",
        "results = model2.fit(X_train, y_train_enc, epochs=50, batch_size=None, verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SaDapGKvxFH7"
      },
      "outputs": [],
      "source": [
        "relu_loss = results.history['loss']\n",
        "relu_accuracy = results.history['accuracy']\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
        "sns.lineplot(x=results.epoch, y=relu_loss, ax=ax1, label='loss')\n",
        "sns.lineplot(x=results.epoch, y=relu_accuracy, ax=ax2, label='accuracy');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NU3W3H4GxFH8"
      },
      "source": [
        "### Adding in Validation Data\n",
        "\n",
        "We have been looking only at our training set. Let's add in our validation set to the picture. Check the docstring for the `.fit()` method and add in our validation data."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_pure_train, X_val, y_pure_train_enc, y_val_enc =\\\n",
        "    train_test_split(X_train, y_train_enc, random_state=42, test_size=0.2)"
      ],
      "metadata": {
        "id": "Fc5EO3hrk8t6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a6erlDahxFH8"
      },
      "outputs": [],
      "source": [
        "model3 = Sequential()\n",
        "model3.add(Dense(12, activation='relu', input_dim=64))\n",
        "model3.add(Dense(8, activation='relu'))\n",
        "model3.add(Dense(4, activation='relu'))\n",
        "model3.add(Dense(10, activation='softmax'))\n",
        "\n",
        "model3.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "\n",
        "results = model3.fit(X_pure_train, y_pure_train_enc,\n",
        "                    validation_data=(X_val, y_val_enc),\n",
        "                    epochs=50,\n",
        "                    batch_size=10)\n",
        "\n",
        "train_loss = results.history['loss']\n",
        "train_acc = results.history['accuracy']\n",
        "val_loss = results.history['val_loss']\n",
        "val_acc = results.history['val_accuracy']\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
        "sns.lineplot(x=results.epoch, y=train_loss, ax=ax1, label='train_loss')\n",
        "sns.lineplot(x=results.epoch, y=train_acc, ax=ax2, label='train_accuracy')\n",
        "\n",
        "sns.lineplot(x=results.epoch, y=val_loss, ax=ax1, label='val_loss')\n",
        "sns.lineplot(x=results.epoch, y=val_acc, ax=ax2, label='val_accuracy');"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LSMkROvzxFH9"
      },
      "outputs": [],
      "source": [
        "results.history['val_accuracy'][-1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wNJEQs0hxFH9"
      },
      "source": [
        "## Connecting with `sklearn`\n",
        "\n",
        "The `keras.wrappers` submodule means that we can turn `keras` models into estimators that `sklearn` tools will recognize."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S2-F5VtJxFH9"
      },
      "outputs": [],
      "source": [
        "# This will throw an error.\n",
        "\n",
        "cross_val_score(model3, X_pure_train, y_pure_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "But if we use the `scikit_learn` wrapper we can take advantage of our familiar scikit-learn tools!"
      ],
      "metadata": {
        "id": "ivgmeNaTHJYk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h1QLMwkpxFH9"
      },
      "outputs": [],
      "source": [
        "def build_model():\n",
        "    model = Sequential()\n",
        "    model.add(Dense(12, activation='relu', input_dim=64))\n",
        "    model.add(Dense(8, activation='relu'))\n",
        "    model.add(Dense(4, activation='relu'))\n",
        "    model.add(Dense(10, activation = 'softmax'))\n",
        "\n",
        "    model.compile(optimizer='adam',\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7srRyJA4xFH9"
      },
      "outputs": [],
      "source": [
        "keras_model = scikit_learn.KerasClassifier(build_model,\n",
        "                                          epochs=50,\n",
        "                                          batch_size=32,\n",
        "                                          verbose=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zog5IV8vxFH9"
      },
      "outputs": [],
      "source": [
        "type(keras_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HAwMJCxixFH9"
      },
      "outputs": [],
      "source": [
        "cross_val_score(keras_model, X_pure_train, y_pure_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qAjcPjYXxFIA"
      },
      "source": [
        "## Other Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s-G_G7cFxFIA"
      },
      "outputs": [],
      "source": [
        "history = results.history\n",
        "training_loss = history['loss']\n",
        "val_loss = history['val_loss']\n",
        "training_accuracy = history['accuracy']\n",
        "val_accuracy = history['val_accuracy']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rxu0xITLxFIB"
      },
      "outputs": [],
      "source": [
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "\n",
        "sns.lineplot(x=list(range(len(training_loss))),\n",
        "             y=training_loss, color='r', label='training', ax=ax1)\n",
        "sns.lineplot(x=list(range(len(val_loss))),\n",
        "             y=val_loss, color='b', label='validation', ax=ax1)\n",
        "sns.lineplot(x=list(range(len(training_loss))),\n",
        "             y=training_accuracy, color='r', label='training',ax=ax2)\n",
        "sns.lineplot(x=list(range(len(val_loss))),\n",
        "             y=val_accuracy, color='b', label='validation',ax=ax2)\n",
        "ax1.legend();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "05jRfBBxxFIB"
      },
      "outputs": [],
      "source": [
        "y_hat_test = np.argmax(model3.predict(X_test), axis=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JdFTVrzNxFIB"
      },
      "outputs": [],
      "source": [
        "confusion_matrix(y_hat_test, y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EdZDyJfARV8l"
      },
      "source": [
        "## Pipelining"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def my_scaler(x):\n",
        "  return x/16"
      ],
      "metadata": {
        "id": "00gtSF-lOzwB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Scaler = FunctionTransformer(my_scaler)"
      ],
      "metadata": {
        "id": "j7xe7ayCQ5OZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "steps = [('scaler', Scaler), ('model', keras_model)]"
      ],
      "metadata": {
        "id": "63Fwg_ATQ_eq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipe = Pipeline(steps=steps)"
      ],
      "metadata": {
        "id": "GOZh74lfRRwy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_raw_train, X_raw_test, y_raw_train, y_raw_test = train_test_split(X, y, random_state=43)"
      ],
      "metadata": {
        "id": "097c23ndxg0S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ohe2 = OneHotEncoder(sparse=False)\n",
        "y_raw_train_enc = ohe2.fit_transform(y_raw_train.reshape(-1, 1))\n",
        "y_raw_test_enc = ohe2.transform(y_raw_test.reshape(-1, 1))"
      ],
      "metadata": {
        "id": "D7H0nrqCx0_V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipe.fit(X_raw_train, y_raw_train_enc,\n",
        "         model__epochs=30, model__validation_data=(X_raw_test, y_raw_test_enc))"
      ],
      "metadata": {
        "id": "_wGbtOxTRTb1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dVFq1AURRW6c"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "celltoolbar": "Raw Cell Format",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}